# ğŸ“œ å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•æ—¶é—´çº¿ï¼ˆ2003â€“2025ï¼‰

æ•´ç†å†…å®¹åŒ…å«ï¼š
- æ¨¡å‹ç»“æ„æ¼”è¿›
- æ¨ç†èƒ½åŠ›å¢å¼ºï¼ˆReasoningï¼‰
- æ¨ç†æ•ˆç‡ä¼˜åŒ–ï¼ˆInference Optimizationï¼‰
- å¤šæ¨¡æ€ä¸æ™ºèƒ½ä½“è¶‹åŠ¿
- å›½å†…å¤§æ¨¡å‹è¿›å±•ä¸ Agent ç”Ÿæ€

---

## ğŸ§± 2003â€“2013ï¼šè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œèµ·ç‚¹

- **2003**ï¼šNNLM  
  ğŸ“„ [Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

- **2010**ï¼šRNNLM  
  ğŸ“„ [Recurrent Neural Network Based Language Model](https://www.fit.vutbr.cz/research/view_pub.php?id=9848)

- **2013**ï¼šWord2Vec  
  ğŸ“„ [Efficient Estimation of Word Representations](https://arxiv.org/abs/1301.3781)

---

## ğŸ§  2014â€“2016ï¼šè¯å‘é‡ä¸ä¸Šä¸‹æ–‡å»ºæ¨¡

- **2014**ï¼šGloVe  
  ğŸ“„ [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)

- **2016**ï¼šFastText  
  ğŸ“„ [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)

---

## ğŸ§  2017â€“2018ï¼šTransformer ä¸ä¸Šä¸‹æ–‡é©å‘½

- **2017**ï¼šTransformer  
  ğŸ“„ [Attention is All You Need](https://arxiv.org/abs/1706.03762)

- **2018**ï¼šELMo  
  ğŸ“„ [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365)

- **2018**ï¼šGPT-1  
  ğŸ“„ [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

- **2018**ï¼šBERT  
  ğŸ“„ [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)

---

## ğŸš€ 2019â€“2020ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çˆ†å‘

- **2019**ï¼šGPT-2  
  ğŸ“„ [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

- **2019**ï¼šXLNet  
  ğŸ“„ [XLNet: Generalized Autoregressive Pretraining](https://arxiv.org/abs/1906.08237)

- **2019**ï¼šRoBERTa  
  ğŸ“„ [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)

- **2020**ï¼šT5  
  ğŸ“„ [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)

- **2020**ï¼šELECTRA  
  ğŸ“„ [ELECTRA: Pre-training Text Encoders as Discriminators](https://arxiv.org/abs/2003.10555)

---

## ğŸ§  2020â€“2021ï¼šå¤§æ¨¡å‹ä¸ Few-shot èƒ½åŠ›

- **2020**ï¼šGPT-3  
  ğŸ“„ [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

- **2021**ï¼šSwitch Transformer  
  ğŸ“„ [Switch Transformers: Scaling to Trillion Parameter Models](https://arxiv.org/abs/2101.03961)

- **2021**ï¼šGShard / GLaM  
  ğŸ“„ [GShard](https://arxiv.org/abs/2006.16668), [GLaM](https://arxiv.org/abs/2112.06905)

- **2021**ï¼šé¹åŸÂ·ç›˜å¤ï¼ˆPanGu-Î±ï¼‰  
  âœ… ä¸­å›½é¦–ä¸ªå¼€æºåƒäº¿çº§ä¸­æ–‡å¤§æ¨¡å‹ï¼Œç”±åä¸º/é¹åŸå®éªŒå®¤å‘å¸ƒ

---

## ğŸ§¾ 2022ï¼šæŒ‡ä»¤å¾®è°ƒä¸æ¨ç†èƒ½åŠ›èŒèŠ½

- **InstructGPT**  
  ğŸ“„ [Training language models to follow instructions](https://arxiv.org/abs/2203.02155)

- **FLAN-T5**  
  ğŸ“„ [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

- **Chain-of-Thought Prompting**  
  ğŸ“„ [Chain of Thought Prompting Elicits Reasoning](https://arxiv.org/abs/2201.11903)

- **Self-Consistency**  
  ğŸ“„ [Self-Consistency Improves Chain of Thought Reasoning](https://arxiv.org/abs/2203.11171)

- **FlashAttention**  
  ğŸ“„ [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)

- **ERNIE 3.0 Titanï¼ˆç™¾åº¦ï¼‰**  
  âœ… ä¸­è‹±åŒè¯­æ¨¡å‹ï¼ˆ260Bï¼‰ï¼ŒçŸ¥è¯†å¢å¼ºé¢„è®­ç»ƒæ¡†æ¶ï¼Œåˆ·æ–°54é¡¹NLPä»»åŠ¡

- **GLMï¼ˆæ¸…åï¼‰**  
  ğŸ“„ [GLM: General Language Model Pretraining](https://arxiv.org/abs/2103.10360)  
  âœ… æˆä¸º ChatGLM ç³»åˆ—åŸºç¡€

---

## ğŸ¤– 2023ï¼šå¼€æºæ¨¡å‹çˆ†å‘ + æ¨ç†èƒ½åŠ›å‡çº§

- **ChatGPT**  
  ğŸ“„ [ChatGPT Blog](https://openai.com/blog/chatgpt)

- **Alpaca**  
  ğŸ“„ [Alpaca Blog](https://crfm.stanford.edu/2023/03/13/alpaca.html)

- **LLaMAï¼ˆMetaï¼‰**  
  âœ… å¼•çˆ†å…¨çƒå¼€æºç¤¾åŒºï¼ˆAlpaca/Vicunaç­‰å‡åŸºäºæ­¤ï¼‰

- **ChatGLM2-6Bï¼ˆæ¸…åæ™ºè°±ï¼‰**  
  âœ… æ”¯æŒ32Kä¸Šä¸‹æ–‡ï¼Œä¸­è‹±åŒè¯­å¼€æºå¯¹è¯æ¨¡å‹

- **Qwenï¼ˆé˜¿é‡Œé€šä¹‰ï¼‰**  
  ğŸ“„ [Qwen Technical Report](https://huggingface.co/Qwen)  
  âœ… æ¨å‡ºQwen-7Bä¸Qwen-72Bï¼ˆä¸­è‹±æœ€å¼ºå¼€æºæ¨¡å‹ä¹‹ä¸€ï¼‰

- **Baichuanï¼ˆç™¾å·æ™ºèƒ½ï¼‰**  
  ğŸ“„ [Baichuan 2](https://huggingface.co/baichuan-inc)  
  âœ… å¼€æº Baichuan-7B/13Bï¼Œå‡çº§ Baichuan2 ç³»åˆ—

- **DeepSeek LLMï¼ˆæ·±åº¦æ±‚ç´¢ï¼‰**  
  ğŸ“„ [DeepSeek Technical Report](https://github.com/deepseek-ai)  
  âœ… æ”¯æŒ128Kä¸Šä¸‹æ–‡ï¼Œä¸­è‹±æ€§èƒ½é¢†å…ˆ

- **Tree-of-Thoughts (ToT)**  
  ğŸ“„ [Tree of Thoughts](https://arxiv.org/abs/2305.10601)

- **ReAct**  
  ğŸ“„ [ReAct: Reasoning and Acting](https://arxiv.org/abs/2210.03629)

- **Speculative Decoding**  
  ğŸ“„ [Speculative Sampling](https://arxiv.org/abs/2302.01318)

- **QLoRA**  
  ğŸ“„ [QLoRA: Efficient Finetuning](https://arxiv.org/abs/2305.14314)

- **vLLM**  
  ğŸ“„ [vLLM](https://arxiv.org/abs/2309.06180)

- **VisualGLMï¼ˆæ¸…åï¼‰**  
  âœ… å¼€æºå›¾æ–‡å¯¹è¯æ¨¡å‹

---

## ğŸ¯ 2024ï¼šæ¨ç†ç­–ç•¥æˆç†Ÿ + å¤šæ¨¡æ€èåˆ

- **Think Twice**  
  ğŸ“„ [Think Twice](https://arxiv.org/abs/2309.00653)

- **Reflection**  
  ğŸ“„ [Self-Reflection](https://arxiv.org/abs/2302.12813)

- **GPT-4o**  
  ğŸ“„ [GPT-4o Blog](https://openai.com/index/gpt-4o/)

- **Claude 3 / Gemini 1.5**  
  ğŸ“„ [Claude](https://www.anthropic.com/index/introducing-claude), [Gemini](https://deepmind.google/technologies/gemini/)

- **Qwen1.5 ç³»åˆ—**  
  âœ… æ”¯æŒ MoE æ¶æ„ï¼Œæ¨ç†æ•ˆç‡æå‡75%

- **DeepSeek-VL**  
  âœ… å›¾æ–‡ç†è§£ + æ–‡æ¡£è§£ææ¨¡å‹ï¼ˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼‰

- **Yiï¼ˆ01.AIï¼‰**  
  ğŸ“„ [Yi: Open Foundation Models](https://huggingface.co/01-ai)  
  âœ… Yi-1.5 ç³»åˆ—ç™»é¡¶ HuggingFace å¼€æºæ¦œ

- **CogVLMï¼ˆæ™ºè°±ï¼‰**  
  âœ… å›¾æ–‡ç†è§£èƒ½åŠ›é€¼è¿‘ GPT-4V

- **Yi-VLï¼ˆ01.AIï¼‰**  
  âœ… å¤šæ¨¡æ€æ¨¡å‹ Yi-VL å‘å¸ƒ

- **Megablocks**  
  ğŸ“„ [Megablocks: Efficient MoE Inference](https://arxiv.org/abs/2401.08722)

- **DeepSeek-RWKV**  
  âœ… åŸºäº RNN æ¶æ„çš„ç™¾ä¸‡ä¸Šä¸‹æ–‡æ¨¡å‹

---

## ğŸ¤– 2025ï¼šAgent æ™ºèƒ½ä½“ä¸æŒç»­æ¨ç†ä¼˜åŒ–

- **LangGraph / AutoGPT**  
  ğŸ“„ [LangGraph](https://www.langgraph.dev/), [AutoGPT](https://github.com/Torantulino/Auto-GPT)

- **Qwen-Agentï¼ˆé˜¿é‡Œï¼‰**  
  âœ… é€šä¹‰å¤šæ¨¡æ€ Agent æ¡†æ¶ï¼Œæ”¯æŒæµè§ˆå™¨ã€ä»£ç è§£é‡Šå™¨ç­‰å·¥å…·è°ƒç”¨

- **DeepSeek-Agent**  
  âœ… å…·å¤‡é•¿æœŸè®°å¿†ä¸è§„åˆ’èƒ½åŠ›ï¼ˆ2025å¹´3æœˆå¼€æºï¼‰

- **OpenAgentsï¼ˆæ¸…åï¼‰**  
  ğŸ“„ [OpenAgents](https://github.com/OpenAgents)  
  âœ… å¼€æºçœŸå®ç¯å¢ƒ Agent å¹³å°ï¼Œæ”¯æŒ 200+ å·¥å…·è°ƒç”¨

---

